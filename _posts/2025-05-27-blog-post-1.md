---
title: 'Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape'
date: 2025-05-27
permalink: /posts/2025/05/blog-post-1/
tags:
  - SAM
  - LoRA
---

Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape
======

# Abstract

* relationship between LoRA optimization space and full parameter space is overlooked

* random perturbation generation strategy for imporved performance and carefully manage memory overhead using random seeds

* experiments: mathematical reasoning, coding abilities, dialogue generation, instruction following, text-to-image generation

# Introduction

![img](../images/flat_lora.png)

LoRA constains optimization to a much lower-dimensional space, and its performance depends on how solutions in this restricted space relate to the full parameter space.

$$\min_{A,B}\max_{\|\varepsilon_W\|_F\leq\rho}L(W+BA+\varepsilon_W)$$

* doubles training cost

* requires storing an additional perturbation 

$$\min_{A,B}\quad\mathbb{E}_{(\varepsilon_W)_{i,j}\sim\mathcal{N}(0,\sigma^2)}\quad L(W+BA+\varepsilon_W)$$