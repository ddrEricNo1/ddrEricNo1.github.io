---
title: 'Transformed Low-Rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models'
date: 2025-05-29
permalink: /posts/2025/05/blog-post-2/
tags:
  - LoRA
  - Stable Diffusion
---

Transformed Low-Rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models
======

# Abstract

## Research Background

Approximation gap between low-rank assumption and desired fine-tuning weights prevents the simultaneous acquisition of ultra-parameter efficiency and better performance.

## Reserach Objective

By combining **transform** and **residual adaptations**, to imporve parameter efficiency and performance. 

## Invention points

* **transform**: align pre-trained weights as close as possible to the desired weights

* **residual adaptation**: be effectively appropriated by more compact and parameter-efficient structures

* summarize other PEFT methods using **transform** and **residual adaptation** architecture

## Experiments scope

fine-tune **stable diffusion** models.

# Introduction

![img](../images/flat_lora.png)

LoRA constains optimization to a much lower-dimensional space, and its performance depends on how solutions in this restricted space relate to the full parameter space.

$$\min_{A,B}\max_{\|\varepsilon_W\|_F\leq\rho}L(W+BA+\varepsilon_W)$$

* doubles training cost

* requires storing an additional perturbation 

$$\min_{A,B}\quad\mathbb{E}_{(\varepsilon_W)_{i,j}\sim\mathcal{N}(0,\sigma^2)}\quad L(W+BA+\varepsilon_W)$$