---
title: 'Review of the paper 《Attention is All you Need》'
date: 2024-07-31
permalink: /posts/2024/07/blog-post-1/
tags:
  - NLP
  - Transformer
  - Sequence Transductive Models
---

Attention is All you Need
=====

## Introduction

Dominant **sequence transduction models (For example, machine translation: generate another sequence from a given sequence)** are based on recurrent or convolutional neural networks that include an encoder and a decoder. 

**Problems of recurrent models**: They generate a sequence of hidden state $h_t$ as a function of the previous hidden state $h\_{t-1}$ and the input for position $t$. (inefficient for parallel computing)

**Transformer**: 

* based solely on attention mechanisms

* more parallelizable and require significantly less time to train

---


