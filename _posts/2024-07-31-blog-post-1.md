---
title: 'Review of the paper 《Attention is All you Need》'
date: 2024-07-31
permalink: /posts/2024/07/blog-post-1/
tags:
  - NLP
  - Transformer
  - Sequence Transductive Models
---

Attention is All you Need
=====

## Introduction

Dominant **sequence transduction models (For example, machine translation: generate another sequence from a given sequence)** are based on recurrent or convolutional neural networks that include an encoder and a decoder.

**Problems of recurrent models**: They generate a sequence of hidden state \\(h_t\\) as a function of the previous hidden state \\(h_{t-1}\\) and the input for position \\(t\\). (inefficient for parallel computing)

**Transformer**: 

* based solely on attention mechanisms (eschewing recurrence and relying entirely on attention mechanism to draw dependencies between input and output)

* more **parallelizable** and require significantly less time to train

### What is encoder-decoder architecture?

the encoder maps an input sequence of symbol representations \\(x_1, \cdots, x_n\\) to a sequence of continuous representations \\(\textbf{z}=(z_1, \cdots, z_n)\\)
## Model Architecture


---


